{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1: Self-Play\n",
    "**Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?**\n",
    "\n",
    "Given losses and draws are the same, and that tic-tac-toe devolves to a state where both participants can never lose (only draw) if they play perfectly, then both agents will converge to a perfect play state but continue to explore randomly around this state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.2: Symmetries\n",
    "**Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what ways would this improve it?**\n",
    "\n",
    "We can amend the observation of the state provided to the agent such that symmetric states always appear as the same observation to the agent. The agent could learn faster as it doesn't have to waste time re-learning the value of two states which are in this sense \"identical\".\n",
    "\n",
    "**Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?**\n",
    "\n",
    "No. If we take advantage of symmetries then we lose the opportunity to learn a class of flaws that may be present in a flawed opponent. If such a flaw is present in the opponent, then symmetrically equivalent positions do not have the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3: Greedy Play\n",
    "**Suppose the reinforcement learning player was *greedy*, that is, it always played the move that bought it to the position that it rated the best. Would it learn to play better, or worse, than a nongreedy player? What problems might occur?**\n",
    "\n",
    "Worse.\n",
    "\n",
    "Assume we're playing against a fixed opponent. A non-greedy agent, given enough time, should encounter every state-action combination, and hence correctly *value* every state-action combination. A greedy agent has no guarantee. The failure mode specifically looks like: Greedy Agent estimates the value V of action X in a given state S as greater than the value of all other actions given S after some learning episode I, the estimate of V happens to be close to true V and it is not expected to change drastically (this part is conditional on how the update of V takes place, but assume small step-size and using the TD learning method outlined on pg 13). Greedy Agent now always takes action X given S in all subsequent episodes, even though the true V of action Y given S may be higher than true V of X given S.\n",
    "\n",
    "Against a changing opponent I still expect a greedy agent to perform worse. A changing opponent is just a fixed opponent for a limited set of episodes. So the question is, which agent is expected to converge to optimal play faster given an arbitrary initial state? A greedy agent still suffers the same problem detailed above and may get *locked* (or already be in a locked initial state), so we expect it to converge slower (if ever)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.4: Learning from Exploration\n",
    "**Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves?**\n",
    "\n",
    "Each probability for state S is:\n",
    "- If you learn from exploratory moves, P\\[Win | x*(take random action in state S) * (1 - x)(take optimal action in state S)\\] where x is probability to explore at each time-step\n",
    "- If you don't learn from exploratory moves, P\\[Win | take optimal action in state S]\n",
    "\n",
    "i.e. the Value of each state either bakes in how taking a random exploration move affects your win probability or not.\n",
    "\n",
    "If you continue to explore indefinitely it would be better to learn off the former set of probabilities in order to maximise win rate. For tic-tac-toe specifically though I think this does not matter, it will reduce the V of the entire state-action space in a way that means you still end up taking the same actions. In another game there might be a policy which is more optimal given desire for indefinite exploration. e.g. The policy could make you take actions which *hedge* your exposure to taking stupid future actions (i.e. exploring) in a way which maximise total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.5: Other Improvements\n",
    "**Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?**\n",
    "\n",
    "Against a fixed player, no. Against a changing player it will ultimately come down to identifying how the player changes and how drastically and adjusting your update parameters to balance exploration/exploitation accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
